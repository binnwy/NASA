{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1iHoVgIeQWDhNTrirPtiDF8kpe_YxxxaR",
      "authorship_tag": "ABX9TyPJsR4E4ZvGLlg8IdUvW7RW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binnwy/NASA/blob/main/Exo_Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/exo/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQYoWE2P8xkl",
        "outputId": "c021ea02-709b-4eff-c8b1-e0a9adde8353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "autoencoder_kepler_confirmed.h5    kepler_koi_cleaned.csv\n",
            "autoencoder_kepler.h5\t\t   rf_kepler_model.joblib\n",
            "autoencoder_scaler_confirmed.save  svm_kepler_model.save\n",
            "autoencoder_scaler.save\t\t   svm_kepler_no_smote_model.save\n",
            "confirmed_exoplanets.csv\t   svm_kepler_tuned_model.save\n",
            "dnn_kepler.keras\t\t   svm_scaler_no_smote.save\n",
            "dnn_scaler.save\t\t\t   svm_scaler.save\n",
            "dt_kepler_model.joblib\t\t   xgb_kepler_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/exo/kepler_koi_cleaned.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "A6smZPTBC8rT",
        "outputId": "8ab3de92-9045-4677-acb6-87cbd23b4409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   koi_score  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  \\\n",
              "0      1.000              0              0              0              0   \n",
              "1      0.969              0              0              0              0   \n",
              "2      0.000              0              0              0              0   \n",
              "3      0.000              0              1              0              0   \n",
              "4      1.000              0              0              0              0   \n",
              "\n",
              "   koi_period  koi_duration  koi_depth  koi_prad  koi_impact  koi_steff  \\\n",
              "0    9.488036       2.95750      615.8      2.26       0.146     5455.0   \n",
              "1   54.418383       4.50700      874.8      2.83       0.586     5455.0   \n",
              "2   19.899140       1.78220    10829.0     14.60       0.969     5853.0   \n",
              "3    1.736952       2.40641     8079.2     33.46       1.276     5805.0   \n",
              "4    2.525592       1.65450      603.3      2.75       0.701     6031.0   \n",
              "\n",
              "   koi_slogg  koi_srad  koi_kepmag koi_disposition  koi_disposition_encoded  \n",
              "0      4.467     0.927      15.347       CONFIRMED                        1  \n",
              "1      4.467     0.927      15.347       CONFIRMED                        1  \n",
              "2      4.544     0.868      15.436       CANDIDATE                        0  \n",
              "3      4.564     0.791      15.597  FALSE POSITIVE                       -1  \n",
              "4      4.438     1.046      15.509       CONFIRMED                        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-abd80a36-0ba9-4a08-94ca-360c265f7ac9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>koi_score</th>\n",
              "      <th>koi_fpflag_nt</th>\n",
              "      <th>koi_fpflag_ss</th>\n",
              "      <th>koi_fpflag_co</th>\n",
              "      <th>koi_fpflag_ec</th>\n",
              "      <th>koi_period</th>\n",
              "      <th>koi_duration</th>\n",
              "      <th>koi_depth</th>\n",
              "      <th>koi_prad</th>\n",
              "      <th>koi_impact</th>\n",
              "      <th>koi_steff</th>\n",
              "      <th>koi_slogg</th>\n",
              "      <th>koi_srad</th>\n",
              "      <th>koi_kepmag</th>\n",
              "      <th>koi_disposition</th>\n",
              "      <th>koi_disposition_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.488036</td>\n",
              "      <td>2.95750</td>\n",
              "      <td>615.8</td>\n",
              "      <td>2.26</td>\n",
              "      <td>0.146</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>4.467</td>\n",
              "      <td>0.927</td>\n",
              "      <td>15.347</td>\n",
              "      <td>CONFIRMED</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.969</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>54.418383</td>\n",
              "      <td>4.50700</td>\n",
              "      <td>874.8</td>\n",
              "      <td>2.83</td>\n",
              "      <td>0.586</td>\n",
              "      <td>5455.0</td>\n",
              "      <td>4.467</td>\n",
              "      <td>0.927</td>\n",
              "      <td>15.347</td>\n",
              "      <td>CONFIRMED</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>19.899140</td>\n",
              "      <td>1.78220</td>\n",
              "      <td>10829.0</td>\n",
              "      <td>14.60</td>\n",
              "      <td>0.969</td>\n",
              "      <td>5853.0</td>\n",
              "      <td>4.544</td>\n",
              "      <td>0.868</td>\n",
              "      <td>15.436</td>\n",
              "      <td>CANDIDATE</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.736952</td>\n",
              "      <td>2.40641</td>\n",
              "      <td>8079.2</td>\n",
              "      <td>33.46</td>\n",
              "      <td>1.276</td>\n",
              "      <td>5805.0</td>\n",
              "      <td>4.564</td>\n",
              "      <td>0.791</td>\n",
              "      <td>15.597</td>\n",
              "      <td>FALSE POSITIVE</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.525592</td>\n",
              "      <td>1.65450</td>\n",
              "      <td>603.3</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.701</td>\n",
              "      <td>6031.0</td>\n",
              "      <td>4.438</td>\n",
              "      <td>1.046</td>\n",
              "      <td>15.509</td>\n",
              "      <td>CONFIRMED</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abd80a36-0ba9-4a08-94ca-360c265f7ac9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-abd80a36-0ba9-4a08-94ca-360c265f7ac9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-abd80a36-0ba9-4a08-94ca-360c265f7ac9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-74b06950-54a2-49be-a011-ff1a8016129c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-74b06950-54a2-49be-a011-ff1a8016129c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-74b06950-54a2-49be-a011-ff1a8016129c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7994,\n  \"fields\": [\n    {\n      \"column\": \"koi_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4770091643189129,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 649,\n        \"samples\": [\n          0.34,\n          0.024,\n          0.251\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_fpflag_nt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 0,\n        \"max\": 465,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          465\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_fpflag_ss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_fpflag_co\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_fpflag_ec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_period\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 86.67749146887121,\n        \"min\": 0.259819659,\n        \"max\": 1071.232624,\n        \"num_unique_values\": 7994,\n        \"samples\": [\n          11.4927112,\n          31.578651216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.3095630296359895,\n        \"min\": 0.052,\n        \"max\": 138.54,\n        \"num_unique_values\": 6950,\n        \"samples\": [\n          7.08223,\n          12.1317\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_depth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 85048.28698402985,\n        \"min\": 0.0,\n        \"max\": 921670.0,\n        \"num_unique_values\": 6336,\n        \"samples\": [\n          176.6,\n          215.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_prad\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 312.092199456585,\n        \"min\": 0.08,\n        \"max\": 26042.9,\n        \"num_unique_values\": 2831,\n        \"samples\": [\n          61.16,\n          26.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_impact\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7754802784080395,\n        \"min\": 0.0,\n        \"max\": 25.224,\n        \"num_unique_values\": 1391,\n        \"samples\": [\n          1.287,\n          0.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_steff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 788.3220565293493,\n        \"min\": 2661.0,\n        \"max\": 15896.0,\n        \"num_unique_values\": 2288,\n        \"samples\": [\n          6095.0,\n          4094.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_slogg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.42441284865512213,\n        \"min\": 0.047,\n        \"max\": 5.364,\n        \"num_unique_values\": 1438,\n        \"samples\": [\n          4.551,\n          3.95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_srad\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.422343144827839,\n        \"min\": 0.109,\n        \"max\": 229.908,\n        \"num_unique_values\": 2075,\n        \"samples\": [\n          1.263,\n          0.527\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_kepmag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.3669265256379857,\n        \"min\": 6.966,\n        \"max\": 20.003,\n        \"num_unique_values\": 3544,\n        \"samples\": [\n          15.923,\n          14.246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_disposition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"CONFIRMED\",\n          \"CANDIDATE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"koi_disposition_encoded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 1,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuafxW-q5Sr-",
        "outputId": "930d5d56-1036-4537-8dfe-d970db5c87e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fully self-contained ensemble saved!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "from collections import Counter\n",
        "\n",
        "# Load base models in Colab\n",
        "model_svm = joblib.load(\"/content/drive/MyDrive/exo/svm_kepler_model.save\")\n",
        "scaler_svm = joblib.load(\"/content/drive/MyDrive/exo/svm_scaler_no_smote.save\")\n",
        "\n",
        "model_dnn = load_model(\"/content/drive/MyDrive/exo/dnn_kepler.keras\")\n",
        "scaler_dnn = joblib.load(\"/content/drive/MyDrive/exo/dnn_scaler.save\")\n",
        "\n",
        "model_dt = joblib.load(\"/content/drive/MyDrive/exo/dt_kepler_model.joblib\")\n",
        "model_rf = joblib.load(\"/content/drive/MyDrive/exo/rf_kepler_model.joblib\")\n",
        "model_xgb = joblib.load(\"/content/drive/MyDrive/exo/xgb_kepler_model.joblib\")\n",
        "\n",
        "# Ensemble class\n",
        "class EnsembleModel:\n",
        "    def __init__(self):\n",
        "        # Store models inside the object\n",
        "        self.model_svm = model_svm\n",
        "        self.scaler_svm = scaler_svm\n",
        "        self.model_dnn = model_dnn\n",
        "        self.scaler_dnn = scaler_dnn\n",
        "        self.model_dt = model_dt\n",
        "        self.model_rf = model_rf\n",
        "        self.model_xgb = model_xgb\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        X_svm = self.scaler_svm.transform(X)\n",
        "        X_dnn = self.scaler_dnn.transform(X)\n",
        "\n",
        "        pred_svm = self.model_svm.predict(X_svm)\n",
        "        pred_dnn = (self.model_dnn.predict(X_dnn) > 0.5).astype(int).flatten()\n",
        "        pred_dt = self.model_dt.predict(X)\n",
        "        pred_rf = self.model_rf.predict(X)\n",
        "        pred_xgb = self.model_xgb.predict(X)\n",
        "\n",
        "        from collections import Counter\n",
        "        ensemble_preds = []\n",
        "        for i in range(X.shape[0]):\n",
        "            votes = [pred_svm[i], pred_dnn[i], pred_dt[i], pred_rf[i], pred_xgb[i]]\n",
        "            ensemble_preds.append(Counter(votes).most_common(1)[0][0])\n",
        "        return np.array(ensemble_preds)\n",
        "\n",
        "# Create the object\n",
        "ensemble_model = EnsembleModel()\n",
        "\n",
        "# Save the **fully self-contained ensemble**\n",
        "joblib.dump(ensemble_model, \"/content/drive/MyDrive/exo/ensemble_model_full.save\")\n",
        "print(\"✅ Fully self-contained ensemble saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ensemble model\n",
        "ensemble_model = joblib.load(\"/content/drive/MyDrive/exo/ensemble_model.save\")\n",
        "\n",
        "# Make predictions on new tuples\n",
        "tuple1 = [1, 0, 0, 0, 0, 9.488, 2.95, 615.8, 2.26, 0.146, 5455, 4.467, 0.927, 15.347]\n",
        "tuple2 = [0, 0, 1, 0, 0, 1.736952453, 2.40641, 8079.2, 33.46, 1.276, 5805, 4.564, 0.791, 15.597]\n",
        "\n",
        "preds = ensemble_model.predict([tuple1, tuple2])\n",
        "for i, p in enumerate(preds):\n",
        "    print(f\"Tuple {i+1}: {'Confirmed Planet' if p==1 else 'Not Planet'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UefbJNFD8bUl",
        "outputId": "c8706cc6-c475-421c-84c3-34e63224f635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step\n",
            "Tuple 1: Confirmed Planet\n",
            "Tuple 2: Not Planet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "class ExoplanetEnsemble:\n",
        "    \"\"\"\n",
        "    Ensemble model combining DNN, Decision Tree, Random Forest, and XGBoost\n",
        "    for exoplanet classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model_dnn = None\n",
        "        self.scaler_dnn = None\n",
        "        self.model_dt = None\n",
        "        self.model_rf = None\n",
        "        self.model_xgb = None\n",
        "        self.feature_names = None\n",
        "        self.weights = None\n",
        "\n",
        "    def load_models(self, dnn_path, scaler_path, dt_path, rf_path, xgb_path):\n",
        "        \"\"\"Load all base models and scaler\"\"\"\n",
        "        print(\"Loading models...\")\n",
        "        self.model_dnn = load_model(dnn_path)\n",
        "        self.scaler_dnn = joblib.load(scaler_path)\n",
        "        self.model_dt = joblib.load(dt_path)\n",
        "        self.model_rf = joblib.load(rf_path)\n",
        "        self.model_xgb = joblib.load(xgb_path)\n",
        "        print(\"All models loaded successfully!\")\n",
        "\n",
        "    def set_feature_names(self, feature_names):\n",
        "        \"\"\"Set the feature names used for training\"\"\"\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def set_weights(self, weights=None):\n",
        "        \"\"\"\n",
        "        Set voting weights for each model\n",
        "        Default: Equal weights [0.25, 0.25, 0.25, 0.25]\n",
        "        Order: [DNN, DT, RF, XGBoost]\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            self.weights = np.array([0.25, 0.25, 0.25, 0.25])\n",
        "        else:\n",
        "            self.weights = np.array(weights)\n",
        "            # Normalize weights to sum to 1\n",
        "            self.weights = self.weights / self.weights.sum()\n",
        "        print(f\"Model weights set: DNN={self.weights[0]:.3f}, DT={self.weights[1]:.3f}, \"\n",
        "              f\"RF={self.weights[2]:.3f}, XGBoost={self.weights[3]:.3f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities using weighted soft voting\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Input features\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        proba : array of shape (n_samples, n_classes)\n",
        "            Weighted average probabilities\n",
        "        \"\"\"\n",
        "        # Scale features for DNN\n",
        "        X_scaled = self.scaler_dnn.transform(X)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        pred_dnn = self.model_dnn.predict(X_scaled, verbose=0)\n",
        "        pred_dt = self.model_dt.predict_proba(X)\n",
        "        pred_rf = self.model_rf.predict_proba(X)\n",
        "        pred_xgb = self.model_xgb.predict_proba(X)\n",
        "\n",
        "        # Handle different output shapes\n",
        "        # DNN might output single column for binary classification\n",
        "        if pred_dnn.shape[1] == 1:\n",
        "            pred_dnn = np.hstack([1 - pred_dnn, pred_dnn])\n",
        "\n",
        "        # Weighted average\n",
        "        ensemble_proba = (\n",
        "            self.weights[0] * pred_dnn +\n",
        "            self.weights[1] * pred_dt +\n",
        "            self.weights[2] * pred_rf +\n",
        "            self.weights[3] * pred_xgb\n",
        "        )\n",
        "\n",
        "        return ensemble_proba\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Predict class labels\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Input features\n",
        "        threshold : float, default=0.5\n",
        "            Decision threshold for binary classification\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array of shape (n_samples,)\n",
        "            Predicted class labels\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        # Assuming binary or multi-class with class 1 being positive\n",
        "        if proba.shape[1] == 2:\n",
        "            return (proba[:, 1] >= threshold).astype(int)\n",
        "        else:\n",
        "            return np.argmax(proba, axis=1)\n",
        "\n",
        "    def predict_with_confidence(self, X):\n",
        "        \"\"\"\n",
        "        Predict with confidence scores and individual model predictions\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict with keys:\n",
        "            - 'prediction': final ensemble prediction\n",
        "            - 'confidence': confidence score (probability)\n",
        "            - 'individual_predictions': dict of each model's prediction\n",
        "        \"\"\"\n",
        "        X_scaled = self.scaler_dnn.transform(X)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        pred_dnn = self.model_dnn.predict(X_scaled, verbose=0)\n",
        "        pred_dt = self.model_dt.predict_proba(X)\n",
        "        pred_rf = self.model_rf.predict_proba(X)\n",
        "        pred_xgb = self.model_xgb.predict_proba(X)\n",
        "\n",
        "        # Handle DNN output shape\n",
        "        if pred_dnn.shape[1] == 1:\n",
        "            pred_dnn = np.hstack([1 - pred_dnn, pred_dnn])\n",
        "\n",
        "        # Get ensemble prediction\n",
        "        ensemble_proba = self.predict_proba(X)\n",
        "        final_pred = np.argmax(ensemble_proba, axis=1)\n",
        "        confidence = np.max(ensemble_proba, axis=1)\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(X)):\n",
        "            results.append({\n",
        "                'prediction': final_pred[i],\n",
        "                'confidence': confidence[i],\n",
        "                'individual_predictions': {\n",
        "                    'DNN': np.argmax(pred_dnn[i]),\n",
        "                    'Decision_Tree': np.argmax(pred_dt[i]),\n",
        "                    'Random_Forest': np.argmax(pred_rf[i]),\n",
        "                    'XGBoost': np.argmax(pred_xgb[i])\n",
        "                },\n",
        "                'individual_probabilities': {\n",
        "                    'DNN': pred_dnn[i].tolist(),\n",
        "                    'Decision_Tree': pred_dt[i].tolist(),\n",
        "                    'Random_Forest': pred_rf[i].tolist(),\n",
        "                    'XGBoost': pred_xgb[i].tolist()\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return results if len(results) > 1 else results[0]\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save the ensemble model\"\"\"\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"Ensemble model saved to {filepath}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filepath):\n",
        "        \"\"\"Load a saved ensemble model\"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(f\"Ensemble model loaded from {filepath}\")\n",
        "        return model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE: Create and save ensemble model in Colab\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Initialize ensemble\n",
        "ensemble = ExoplanetEnsemble()\n",
        "\n",
        "# 2. Load all base models\n",
        "ensemble.load_models(\n",
        "    dnn_path=\"/content/drive/MyDrive/exo/dnn_kepler.keras\",\n",
        "    scaler_path=\"/content/drive/MyDrive/exo/dnn_scaler.save\",\n",
        "    dt_path=\"/content/drive/MyDrive/exo/dt_kepler_model.joblib\",\n",
        "    rf_path=\"/content/drive/MyDrive/exo/rf_kepler_model.joblib\",\n",
        "    xgb_path=\"/content/drive/MyDrive/exo/xgb_kepler_model.joblib\"\n",
        ")\n",
        "\n",
        "# 3. Set feature names (important for the Streamlit UI)\n",
        "feature_names = [\n",
        "    'koi_score', 'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co',\n",
        "    'koi_fpflag_ec', 'koi_period', 'koi_duration', 'koi_depth',\n",
        "    'koi_prad', 'koi_impact', 'koi_steff', 'koi_slogg',\n",
        "    'koi_srad', 'koi_kepmag'\n",
        "]\n",
        "ensemble.set_feature_names(feature_names)\n",
        "\n",
        "# 4. Set weights (optional - defaults to equal weights)\n",
        "# You can adjust these based on model performance\n",
        "# Example: Give more weight to better performing models\n",
        "ensemble.set_weights([0.30, 0.20, 0.25, 0.25])  # DNN, DT, RF, XGBoost\n",
        "\n",
        "# 5. Save the ensemble model\n",
        "ensemble.save(\"/content/drive/MyDrive/exo/ensemble_model.pkl\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ensemble model created and saved successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 6. Test the ensemble (optional)\n",
        "print(\"\\nTesting ensemble with sample data...\")\n",
        "sample_data = np.array([[\n",
        "    1.0, 0, 0, 0, 0, 9.488036, 2.957506, 15.8,\n",
        "    2.26, 0.146, 5455.0, 4.467, 0.927, 15.347\n",
        "]])\n",
        "\n",
        "result = ensemble.predict_with_confidence(sample_data)\n",
        "print(f\"\\nPrediction: {result['prediction']}\")\n",
        "print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "print(f\"Individual predictions: {result['individual_predictions']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQkxJzEm9VLU",
        "outputId": "bf22958b-53c1-4df5-868f-39d62ad1c608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "All models loaded successfully!\n",
            "Model weights set: DNN=0.300, DT=0.200, RF=0.250, XGBoost=0.250\n",
            "Ensemble model saved to /content/drive/MyDrive/exo/ensemble_model.pkl\n",
            "\n",
            "============================================================\n",
            "Ensemble model created and saved successfully!\n",
            "============================================================\n",
            "\n",
            "Testing ensemble with sample data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction: 1\n",
            "Confidence: 0.5705\n",
            "Individual predictions: {'DNN': np.int64(1), 'Decision_Tree': np.int64(1), 'Random_Forest': np.int64(1), 'XGBoost': np.int64(0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "class ExoplanetEnsemble:\n",
        "    \"\"\"\n",
        "    Ensemble model combining DNN, SVM, Decision Tree, Random Forest, and XGBoost\n",
        "    for exoplanet classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model_dnn = None\n",
        "        self.scaler_dnn = None\n",
        "        self.model_svm = None\n",
        "        self.scaler_svm = None\n",
        "        self.model_dt = None\n",
        "        self.model_rf = None\n",
        "        self.model_xgb = None\n",
        "        self.feature_names = None\n",
        "        self.weights = None\n",
        "\n",
        "    def load_models(self, dnn_path, scaler_dnn_path, svm_path, scaler_svm_path,\n",
        "                    dt_path, rf_path, xgb_path):\n",
        "        \"\"\"Load all base models and scalers\"\"\"\n",
        "        print(\"Loading models...\")\n",
        "        self.model_dnn = load_model(dnn_path)\n",
        "        self.scaler_dnn = joblib.load(scaler_dnn_path)\n",
        "        self.model_svm = joblib.load(svm_path)\n",
        "        self.scaler_svm = joblib.load(scaler_svm_path)\n",
        "        self.model_dt = joblib.load(dt_path)\n",
        "        self.model_rf = joblib.load(rf_path)\n",
        "        self.model_xgb = joblib.load(xgb_path)\n",
        "        print(\"All models loaded successfully!\")\n",
        "        print(f\"  ✓ DNN with scaler\")\n",
        "        print(f\"  ✓ SVM with scaler\")\n",
        "        print(f\"  ✓ Decision Tree\")\n",
        "        print(f\"  ✓ Random Forest\")\n",
        "        print(f\"  ✓ XGBoost\")\n",
        "\n",
        "    def set_feature_names(self, feature_names):\n",
        "        \"\"\"Set the feature names used for training\"\"\"\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def set_weights(self, weights=None):\n",
        "        \"\"\"\n",
        "        Set voting weights for each model\n",
        "        Default: Equal weights [0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "        Order: [DNN, SVM, DT, RF, XGBoost]\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            self.weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "        else:\n",
        "            self.weights = np.array(weights)\n",
        "            # Normalize weights to sum to 1\n",
        "            self.weights = self.weights / self.weights.sum()\n",
        "        print(f\"Model weights set: DNN={self.weights[0]:.3f}, SVM={self.weights[1]:.3f}, \"\n",
        "              f\"DT={self.weights[2]:.3f}, RF={self.weights[3]:.3f}, XGBoost={self.weights[4]:.3f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict probabilities using weighted soft voting\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Input features\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        proba : array of shape (n_samples, n_classes)\n",
        "            Weighted average probabilities\n",
        "        \"\"\"\n",
        "        # Scale features for DNN and SVM\n",
        "        X_scaled_dnn = self.scaler_dnn.transform(X)\n",
        "        X_scaled_svm = self.scaler_svm.transform(X)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        pred_dnn = self.model_dnn.predict(X_scaled_dnn, verbose=0)\n",
        "\n",
        "        # SVM prediction - handle both predict_proba and decision_function\n",
        "        if hasattr(self.model_svm, 'predict_proba'):\n",
        "            pred_svm = self.model_svm.predict_proba(X_scaled_svm)\n",
        "        else:\n",
        "            # If SVM doesn't have predict_proba, use decision_function\n",
        "            decision = self.model_svm.decision_function(X_scaled_svm)\n",
        "            # Convert to probabilities using sigmoid\n",
        "            if len(decision.shape) == 1:\n",
        "                # Binary classification\n",
        "                proba_positive = 1 / (1 + np.exp(-decision))\n",
        "                pred_svm = np.column_stack([1 - proba_positive, proba_positive])\n",
        "            else:\n",
        "                # Multi-class\n",
        "                exp_scores = np.exp(decision - np.max(decision, axis=1, keepdims=True))\n",
        "                pred_svm = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        pred_dt = self.model_dt.predict_proba(X)\n",
        "        pred_rf = self.model_rf.predict_proba(X)\n",
        "        pred_xgb = self.model_xgb.predict_proba(X)\n",
        "\n",
        "        # Handle different output shapes\n",
        "        # DNN might output single column for binary classification\n",
        "        if pred_dnn.shape[1] == 1:\n",
        "            pred_dnn = np.hstack([1 - pred_dnn, pred_dnn])\n",
        "\n",
        "        # Ensure all predictions have the same shape\n",
        "        n_classes = max(pred_dnn.shape[1], pred_svm.shape[1], pred_dt.shape[1],\n",
        "                       pred_rf.shape[1], pred_xgb.shape[1])\n",
        "\n",
        "        # Weighted average\n",
        "        ensemble_proba = (\n",
        "            self.weights[0] * pred_dnn +\n",
        "            self.weights[1] * pred_svm +\n",
        "            self.weights[2] * pred_dt +\n",
        "            self.weights[3] * pred_rf +\n",
        "            self.weights[4] * pred_xgb\n",
        "        )\n",
        "\n",
        "        return ensemble_proba\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Predict class labels\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Input features\n",
        "        threshold : float, default=0.5\n",
        "            Decision threshold for binary classification\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : array of shape (n_samples,)\n",
        "            Predicted class labels\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        # Assuming binary or multi-class with class 1 being positive\n",
        "        if proba.shape[1] == 2:\n",
        "            return (proba[:, 1] >= threshold).astype(int)\n",
        "        else:\n",
        "            return np.argmax(proba, axis=1)\n",
        "\n",
        "    def predict_with_confidence(self, X):\n",
        "        \"\"\"\n",
        "        Predict with confidence scores and individual model predictions\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict with keys:\n",
        "            - 'prediction': final ensemble prediction\n",
        "            - 'confidence': confidence score (probability)\n",
        "            - 'individual_predictions': dict of each model's prediction\n",
        "        \"\"\"\n",
        "        X_scaled_dnn = self.scaler_dnn.transform(X)\n",
        "        X_scaled_svm = self.scaler_svm.transform(X)\n",
        "\n",
        "        # Get predictions from all models\n",
        "        pred_dnn = self.model_dnn.predict(X_scaled_dnn, verbose=0)\n",
        "\n",
        "        # SVM prediction\n",
        "        if hasattr(self.model_svm, 'predict_proba'):\n",
        "            pred_svm = self.model_svm.predict_proba(X_scaled_svm)\n",
        "        else:\n",
        "            decision = self.model_svm.decision_function(X_scaled_svm)\n",
        "            if len(decision.shape) == 1:\n",
        "                proba_positive = 1 / (1 + np.exp(-decision))\n",
        "                pred_svm = np.column_stack([1 - proba_positive, proba_positive])\n",
        "            else:\n",
        "                exp_scores = np.exp(decision - np.max(decision, axis=1, keepdims=True))\n",
        "                pred_svm = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        pred_dt = self.model_dt.predict_proba(X)\n",
        "        pred_rf = self.model_rf.predict_proba(X)\n",
        "        pred_xgb = self.model_xgb.predict_proba(X)\n",
        "\n",
        "        # Handle DNN output shape\n",
        "        if pred_dnn.shape[1] == 1:\n",
        "            pred_dnn = np.hstack([1 - pred_dnn, pred_dnn])\n",
        "\n",
        "        # Get ensemble prediction\n",
        "        ensemble_proba = self.predict_proba(X)\n",
        "        final_pred = np.argmax(ensemble_proba, axis=1)\n",
        "        confidence = np.max(ensemble_proba, axis=1)\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(X)):\n",
        "            results.append({\n",
        "                'prediction': final_pred[i],\n",
        "                'confidence': confidence[i],\n",
        "                'individual_predictions': {\n",
        "                    'DNN': np.argmax(pred_dnn[i]),\n",
        "                    'SVM': np.argmax(pred_svm[i]),\n",
        "                    'Decision_Tree': np.argmax(pred_dt[i]),\n",
        "                    'Random_Forest': np.argmax(pred_rf[i]),\n",
        "                    'XGBoost': np.argmax(pred_xgb[i])\n",
        "                },\n",
        "                'individual_probabilities': {\n",
        "                    'DNN': pred_dnn[i].tolist(),\n",
        "                    'SVM': pred_svm[i].tolist(),\n",
        "                    'Decision_Tree': pred_dt[i].tolist(),\n",
        "                    'Random_Forest': pred_rf[i].tolist(),\n",
        "                    'XGBoost': pred_xgb[i].tolist()\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return results if len(results) > 1 else results[0]\n",
        "\n",
        "    def save(self, filepath):\n",
        "        \"\"\"Save the ensemble model\"\"\"\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"Ensemble model saved to {filepath}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load(filepath):\n",
        "        \"\"\"Load a saved ensemble model\"\"\"\n",
        "        with open(filepath, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        print(f\"Ensemble model loaded from {filepath}\")\n",
        "        return model\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE: Create and save ensemble model in Colab\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Initialize ensemble\n",
        "ensemble = ExoplanetEnsemble()\n",
        "\n",
        "# 2. Load all base models (UPDATE THESE PATHS WITH YOUR SVM MODEL PATHS)\n",
        "ensemble.load_models(\n",
        "    dnn_path=\"/content/drive/MyDrive/exo/dnn_kepler.keras\",\n",
        "    scaler_dnn_path=\"/content/drive/MyDrive/exo/dnn_scaler.save\",\n",
        "    svm_path=\"/content/drive/MyDrive/exo/svm_kepler_model.save\",  # UPDATE THIS PATH\n",
        "    scaler_svm_path=\"/content/drive/MyDrive/exo/svm_scaler.save\",  # UPDATE THIS PATH\n",
        "    dt_path=\"/content/drive/MyDrive/exo/dt_kepler_model.joblib\",\n",
        "    rf_path=\"/content/drive/MyDrive/exo/rf_kepler_model.joblib\",\n",
        "    xgb_path=\"/content/drive/MyDrive/exo/xgb_kepler_model.joblib\"\n",
        ")\n",
        "\n",
        "# 3. Set feature names (important for the Streamlit UI)\n",
        "feature_names = [\n",
        "    'koi_score', 'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co',\n",
        "    'koi_fpflag_ec', 'koi_period', 'koi_duration', 'koi_depth',\n",
        "    'koi_prad', 'koi_impact', 'koi_steff', 'koi_slogg',\n",
        "    'koi_srad', 'koi_kepmag'\n",
        "]\n",
        "ensemble.set_feature_names(feature_names)\n",
        "\n",
        "# 4. Set weights (optional - defaults to equal weights)\n",
        "# You can adjust these based on model performance\n",
        "# Example: Give more weight to better performing models\n",
        "ensemble.set_weights([0.25, 0.20, 0.15, 0.20, 0.20])  # DNN, SVM, DT, RF, XGBoost\n",
        "\n",
        "# 5. Save the ensemble model\n",
        "ensemble.save(\"/content/drive/MyDrive/exo/ensemble_model.pkl\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ensemble model created and saved successfully!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 6. Test the ensemble (optional)\n",
        "print(\"\\nTesting ensemble with sample data...\")\n",
        "sample_data = np.array([[\n",
        "    1.0, 0, 0, 0, 0, 9.488036, 2.957506, 15.8,\n",
        "    2.26, 0.146, 5455.0, 4.467, 0.927, 15.347\n",
        "]])\n",
        "\n",
        "result = ensemble.predict_with_confidence(sample_data)\n",
        "print(f\"\\nPrediction: {result['prediction']}\")\n",
        "print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "print(f\"Individual predictions: {result['individual_predictions']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB7nqiGJD2kZ",
        "outputId": "0b11fcf5-5b25-4207-c7c9-eed3cd0121ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "All models loaded successfully!\n",
            "  ✓ DNN with scaler\n",
            "  ✓ SVM with scaler\n",
            "  ✓ Decision Tree\n",
            "  ✓ Random Forest\n",
            "  ✓ XGBoost\n",
            "Model weights set: DNN=0.250, SVM=0.200, DT=0.150, RF=0.200, XGBoost=0.200\n",
            "Ensemble model saved to /content/drive/MyDrive/exo/ensemble_model.pkl\n",
            "\n",
            "============================================================\n",
            "Ensemble model created and saved successfully!\n",
            "============================================================\n",
            "\n",
            "Testing ensemble with sample data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction: 1\n",
            "Confidence: 0.6273\n",
            "Individual predictions: {'DNN': np.int64(1), 'SVM': np.int64(1), 'Decision_Tree': np.int64(1), 'Random_Forest': np.int64(1), 'XGBoost': np.int64(0)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test script to verify the ensemble model works correctly\n",
        "Run this after downloading ensemble_model.pkl to your local system\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def load_and_test_ensemble(model_path='ensemble_model.pkl'):\n",
        "    \"\"\"Load and test the ensemble model\"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"ENSEMBLE MODEL TEST\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load the model\n",
        "    print(\"\\n1. Loading ensemble model...\")\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            ensemble = pickle.load(f)\n",
        "        print(\"   ✅ Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error loading model: {e}\")\n",
        "        return\n",
        "\n",
        "    # Display model info\n",
        "    print(\"\\n2. Model Information:\")\n",
        "    print(f\"   - Feature names: {len(ensemble.feature_names) if ensemble.feature_names else 'Not set'} features\")\n",
        "    if ensemble.feature_names:\n",
        "        print(f\"   - Features: {', '.join(ensemble.feature_names[:5])}...\")\n",
        "    print(f\"   - Model weights:\")\n",
        "    if ensemble.weights is not None:\n",
        "        print(f\"     • DNN: {ensemble.weights[0]:.3f}\")\n",
        "        print(f\"     • SVM: {ensemble.weights[1]:.3f}\")\n",
        "        print(f\"     • Decision Tree: {ensemble.weights[2]:.3f}\")\n",
        "        print(f\"     • Random Forest: {ensemble.weights[3]:.3f}\")\n",
        "        print(f\"     • XGBoost: {ensemble.weights[4]:.3f}\")\n",
        "\n",
        "    # Test with sample data\n",
        "    print(\"\\n3. Testing with sample data...\")\n",
        "\n",
        "    # Test case 1: CONFIRMED exoplanet\n",
        "    test_case_1 = np.array([[\n",
        "        1.0, 0, 0, 0, 0, 9.488036, 2.957506, 15.8,\n",
        "        2.26, 0.146, 5455.0, 4.467, 0.927, 15.347\n",
        "    ]])\n",
        "\n",
        "    # Test case 2: CANDIDATE\n",
        "    test_case_2 = np.array([[\n",
        "        0.969, 0, 0, 0, 0, 54.418383, 4.507008, 74.8,\n",
        "        2.83, 0.586, 5455.0, 4.467, 0.927, 15.347\n",
        "    ]])\n",
        "\n",
        "    # Test case 3: FALSE POSITIVE\n",
        "    test_case_3 = np.array([[\n",
        "        0.0001, 0, 0, 1, 0, 1.736952, 2.406418, 079.2,\n",
        "        33.46, 1.276, 5805.0, 4.564, 0.791, 15.597\n",
        "    ]])\n",
        "\n",
        "    test_cases = [\n",
        "        (\"Test Case 1 (Expected: CONFIRMED)\", test_case_1),\n",
        "        (\"Test Case 2 (Expected: CANDIDATE)\", test_case_2),\n",
        "        (\"Test Case 3 (Expected: FALSE POSITIVE)\", test_case_3)\n",
        "    ]\n",
        "\n",
        "    prediction_map = {-1: \"FALSE POSITIVE\", 0: \"CANDIDATE\", 1: \"CONFIRMED\"}\n",
        "\n",
        "    for test_name, test_data in test_cases:\n",
        "        print(f\"\\n   {test_name}\")\n",
        "        try:\n",
        "            result = ensemble.predict_with_confidence(test_data)\n",
        "\n",
        "            pred_label = prediction_map.get(result['prediction'], \"UNKNOWN\")\n",
        "            print(f\"   └─ Prediction: {pred_label}\")\n",
        "            print(f\"   └─ Confidence: {result['confidence']:.2%}\")\n",
        "            print(f\"   └─ Individual predictions:\")\n",
        "            for model_name, pred in result['individual_predictions'].items():\n",
        "                model_pred = prediction_map.get(pred, \"UNKNOWN\")\n",
        "                print(f\"      • {model_name}: {model_pred}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   └─ ❌ Error during prediction: {e}\")\n",
        "\n",
        "    # Test batch prediction\n",
        "    print(\"\\n4. Testing batch prediction...\")\n",
        "    try:\n",
        "        batch_data = np.vstack([test_case_1, test_case_2, test_case_3])\n",
        "        predictions = ensemble.predict(batch_data)\n",
        "        probabilities = ensemble.predict_proba(batch_data)\n",
        "\n",
        "        print(f\"   ✅ Batch prediction successful!\")\n",
        "        print(f\"   - Predictions: {[prediction_map.get(p, 'UNKNOWN') for p in predictions]}\")\n",
        "        print(f\"   - Probabilities shape: {probabilities.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Error during batch prediction: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST COMPLETED\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\n✅ If all tests passed, your ensemble model is ready for Streamlit!\")\n",
        "    print(\"   Run: streamlit run streamlit_app.py\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the model\n",
        "    load_and_test_ensemble('ensemble_model.pkl')\n",
        "\n",
        "    # Optional: Test with custom data\n",
        "    print(\"\\n\\nWant to test with custom data? (y/n): \", end=\"\")\n",
        "    choice = input().strip().lower()\n",
        "\n",
        "    if choice == 'y':\n",
        "        print(\"\\nEnter feature values (comma-separated):\")\n",
        "        print(\"Format: koi_score,koi_fpflag_nt,koi_fpflag_ss,koi_fpflag_co,koi_fpflag_ec,\")\n",
        "        print(\"        koi_period,koi_duration,koi_depth,koi_prad,koi_impact,\")\n",
        "        print(\"        koi_steff,koi_slogg,koi_srad,koi_kepmag\")\n",
        "\n",
        "        try:\n",
        "            user_input = input(\"\\nValues: \").strip()\n",
        "            values = [float(x.strip()) for x in user_input.split(',')]\n",
        "\n",
        "            if len(values) != 14:\n",
        "                print(f\"❌ Error: Expected 14 values, got {len(values)}\")\n",
        "            else:\n",
        "                custom_data = np.array([values])\n",
        "\n",
        "                with open('ensemble_model.pkl', 'rb') as f:\n",
        "                    ensemble = pickle.load(f)\n",
        "\n",
        "                result = ensemble.predict_with_confidence(custom_data)\n",
        "                prediction_map = {-1: \"FALSE POSITIVE\", 0: \"CANDIDATE\", 1: \"CONFIRMED\"}\n",
        "\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"CUSTOM PREDICTION RESULT\")\n",
        "                print(\"=\"*60)\n",
        "                print(f\"Prediction: {prediction_map.get(result['prediction'], 'UNKNOWN')}\")\n",
        "                print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "                print(\"\\nIndividual Model Predictions:\")\n",
        "                for model_name, pred in result['individual_predictions'].items():\n",
        "                    model_pred = prediction_map.get(pred, \"UNKNOWN\")\n",
        "                    print(f\"  • {model_name}: {model_pred}\")\n",
        "                print(\"=\"*60)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Jy629rXQE2_N",
        "outputId": "7a9067cd-1563-4dbf-fa9d-3d3d06207d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ENSEMBLE MODEL TEST\n",
            "============================================================\n",
            "\n",
            "1. Loading ensemble model...\n",
            "   ❌ Error loading model: [Errno 2] No such file or directory: 'ensemble_model.pkl'\n",
            "\n",
            "\n",
            "Want to test with custom data? (y/n): y\n",
            "\n",
            "Enter feature values (comma-separated):\n",
            "Format: koi_score,koi_fpflag_nt,koi_fpflag_ss,koi_fpflag_co,koi_fpflag_ec,\n",
            "        koi_period,koi_duration,koi_depth,koi_prad,koi_impact,\n",
            "        koi_steff,koi_slogg,koi_srad,koi_kepmag\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3073487049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nValues: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLzWO9scFkci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}